{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################################\n",
    "###### Script for reordering fastMASST results, merging with ReDU and getting information on datasets related to #####\n",
    "###### humans and rodents to build Figure 2 a and b #################################################################\n",
    "######################################################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the fastMASST combined results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to open and concatenate vertically all the fastMASST results. For this, we need to also add\n",
    "# important information about the file (that are in the fileame itself), such as the BA class (mono, di, etc),\n",
    "# and the scan number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the fastmasst summary results (filenames of all the files generated in the workflow)\n",
    "#For more information about this workflow, check https://github.com/robinschmid/microbe_masst\n",
    "\n",
    "result_summary = pd.read_csv('MASST_summary_all.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only rows that contain 'matches' in the first row, to focus separatelly them\n",
    "result_summary_fastmasst = result_summary[result_summary['filename'].str.contains('_matches.tsv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create a loop to read all of these as dataframes, then we create a temporary file, and concatenate them all \n",
    "\n",
    "fastmasst_all = pd.DataFrame()\n",
    "current = 1\n",
    "\n",
    "for i in range(len(result_summary_fastmasst)):\n",
    "    filename = result_summary_fastmasst.iloc[i]['filename']\n",
    "    df_temp = pd.read_csv('/Users/helenarusso/Documents/Pesquisa/UCSD Post-doc/Bile acids mining MassQL/microbeMASST/microbeMASST_bile_acids_all_results.nosync/Non_Refined_OK/'+filename, sep='\\t')\n",
    "    df_temp['Bile acid'] = result_summary_fastmasst.iloc[i]['Bile acid']\n",
    "    df_temp['Scan'] = result_summary_fastmasst.iloc[i]['Scan']\n",
    "    fastmasst_all = pd.concat([fastmasst_all, df_temp], axis=0)\n",
    "    print('Opening {} of {}'.format(current, len(result_summary_fastmasst)))\n",
    "    current += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastmasst_all.to_csv('/Users/helenarusso/Downloads/fastmasst_all_non_refined.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all MassQL results to get the precmz and calculate the delta masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all MassQL data. Cluster index is the scan number of the mgf files sent to for MASST analyses.\n",
    "\n",
    "df_non = pd.read_csv('Nonhydroxy_MSCluster_MCS2_non_refined/Nonhydroxy_75a705dd-view_all_clusters_withID_beta-main.tsv', sep='\\t')\n",
    "df_mono = pd.read_csv('Monohydroxy_MSCluster_MCS2_non_refined/Monohydroxy_88b52c90-view_all_clusters_withID_beta-main.tsv', sep='\\t')\n",
    "df_di = pd.read_csv('Dihydroxy_MSCluster_MCS2_non_refined/Dihydroxy_2frag_11424cb4-view_all_clusters_withID_beta-main.tsv', sep='\\t')\n",
    "df_tri = pd.read_csv('Trihydroxy_MSCluster_MCS2_non_refined/Trihydroxy_85b060ec-view_all_clusters_withID_beta-main.tsv', sep='\\t')\n",
    "df_tetra = pd.read_csv('Tetrahydroxy_MSCluster_MCS2_non_refined/Tetrahydroxy_b51812c1-view_all_clusters_withID_beta-main.tsv', sep='\\t')\n",
    "df_penta = pd.read_csv('Pentahydroxy_MSCluster_MCS2_non_refined/Pentahydroxy_b8404e55-view_all_clusters_withID_beta-main.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating values for a new column to be populated by delta masses. In this case, we first subtract the\n",
    "#precmz column by a proton (assuming that it is a M+H adduct), and then subtract by the uncharged bile acid.\n",
    "\n",
    "df_non[\"delta_mass\"] = (df_non['precursor mass'] - 361.3101)\n",
    "df_mono[\"delta_mass\"] = (df_mono['precursor mass'] - 377.3050)\n",
    "df_di[\"delta_mass\"] = (df_di['precursor mass'] - 393.2999)\n",
    "df_tri[\"delta_mass\"] = (df_tri['precursor mass'] - 409.2949)\n",
    "df_tetra[\"delta_mass\"] = (df_tetra['precursor mass'] - 425.2898)\n",
    "df_penta[\"delta_mass\"] = (df_penta['precursor mass'] - 441.2847)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating absolute numbers (multiply (-1) the negative values)\n",
    "\n",
    "df_non[\"delta_mass\"] = df_non[\"delta_mass\"].abs()\n",
    "df_mono[\"delta_mass\"] = df_mono[\"delta_mass\"].abs()\n",
    "df_di[\"delta_mass\"] = df_di[\"delta_mass\"].abs()\n",
    "df_tri[\"delta_mass\"] = df_tri[\"delta_mass\"].abs()\n",
    "df_tetra[\"delta_mass\"] = df_tetra[\"delta_mass\"].abs()\n",
    "df_penta[\"delta_mass\"] = df_penta[\"delta_mass\"].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we concatenate all the tables vertically\n",
    "\n",
    "df_all = pd.concat([df_non, df_mono, df_di, df_tri, df_tetra, df_penta], axis=0)\n",
    "df_all = df_all.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For us to compare properly, we can have the 'Bile_acid_GNPSjob' + 'cluster index' to merge tables:\n",
    "\n",
    "df_all['Bile_scan'] = df_all['Bile_acid'].astype(str).str.cat(df_all[['cluster index']].astype(str), sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and also do it for the fastMASST table:\n",
    "fastmasst_all_ok = fastmasst_all.copy()\n",
    "fastmasst_all_ok['Bile_scan'] = fastmasst_all['Bile acid'].astype(str).str.cat(fastmasst_all[['Scan']].astype(str),\n",
    "                                                                               sep='_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we merge the two big tables by the Bile_scan column :D\n",
    "merged_tables_all = pd.merge(fastmasst_all_ok, df_all[['Bile_scan', 'precursor mass', 'delta_mass']],\n",
    "                             on = \"Bile_scan\", how = \"left\")\n",
    "\n",
    "#and reset index\n",
    "merged_tables_all = merged_tables_all.reset_index()\n",
    "#Remove the 'index' column as it can get confusing in the middle\n",
    "merged_tables_all = merged_tables_all.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for us to merge with ReDU, we need to have a column with dataset/filename\n",
    "merged_tables_all[\"dataset\"] = merged_tables_all[\"USI\"].str.split(\":\").str[1]\n",
    "merged_tables_all[\"filename\"] = merged_tables_all[\"USI\"].str.split(\":\").str[2]\n",
    "merged_tables_all['filepath'] = merged_tables_all['dataset'].astype(str).str.cat(merged_tables_all[['filename']].astype(str),\n",
    "                                                                                 sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReDU merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ReDU Database (obtained from https://redu.ucsd.edu/ --> Download Database)\n",
    "df_redu = pd.read_csv('/Users/helenarusso/Documents/Pesquisa/UCSD Post-doc/Bile acids mining MassQL/MassQL ok - Histograms.nosync/All/ReDU/all_sampleinformation.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 'f.' from filename column, create new column with the dataset and last element of the filename column, and \n",
    "#remove the extension of the file (.mzML/.mzML) to match the column we have in the other table.\n",
    "df_redu['filename'] = df_redu['filename'].str.replace('f.', '')\n",
    "df_redu['filename_2'] = df_redu['filename'].str.split('/').str[-1]\n",
    "df_redu['filename_2'] = df_redu['filename_2'].str.replace('.mzML', '')\n",
    "df_redu['filename_2'] = df_redu['filename_2'].str.replace('.mzXML', '')\n",
    "df_redu['filepath'] = df_redu['ATTRIBUTE_DatasetAccession'].astype(str).str.cat(df_redu[['filename_2']].astype(str),\n",
    "                                                                                sep='/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge tables from fastmasst and ReDU\n",
    "merged_tables_all_redu = pd.merge(merged_tables_all, df_redu, on='filepath', how='left')\n",
    "merged_tables_all_redu.fillna(\"N/A\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop some columns\n",
    "merged_tables_all_redu = merged_tables_all_redu.drop('filename_2', axis=1)\n",
    "merged_tables_all_redu = merged_tables_all_redu.drop('Status', axis=1)\n",
    "merged_tables_all_redu = merged_tables_all_redu.drop('level_0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export table\n",
    "merged_tables_all_redu.to_csv('/Users/helenarusso/Downloads/fastmasst_all_merged_ReDU_non_refined_final.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's keep only a few columns for clarity\n",
    "\n",
    "merged_tables_all_redu_columns = merged_tables_all_redu[['Bile acid', 'Cosine', 'Matching Peaks', 'Scan', 'USI', \n",
    "                                                         'Bile_scan', 'precursor mass', 'delta_mass', 'dataset',\n",
    "                                                         'filename_x', 'filepath', 'filename_y', 'BiologicalSex', \n",
    "                                                         'HealthStatus', 'LifeStage', 'MassSpectrometer', \n",
    "                                                         'NCBITaxonomy', 'UBERONBodyPartName']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organizing the table for the body part figure (Supplementary Figure S1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step is just keeping the rows that are from datasets related to Homo sapiens\n",
    "\n",
    "merged_tables_all_homo_sapiens = merged_tables_all_redu_columns.loc[merged_tables_all_redu_columns['NCBITaxonomy'] == '9606|Homo sapiens']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check how many unique UBERONBodyPartName there is in the whole data we have\n",
    "merged_tables_all_homo_sapiens[\"UBERONBodyPartName\"].explode().unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with the body part names\n",
    "list_body_homo_sapiens = ['saliva', 'urine', 'skin of trunk', 'blood plasma','skin of manus', 'head or neck skin', \n",
    "                          'arm skin', 'anal region', 'oral cavity', 'feces', 'nasal cavity', 'cerebrospinal fluid',\n",
    "                          'vagina', 'milk', 'brain', 'blood serum', 'not applicable', 'skin of pes', 'skin of body', \n",
    "                          'skin of leg', 'axilla skin']\n",
    "\n",
    "#and add this as rows in the first column of a new dataframe\n",
    "df_body_part_homo_sapiens = pd.DataFrame(columns=['UBERONBodyPartName'])\n",
    "df_body_part_homo_sapiens['UBERONBodyPartName'] = np.array(list_body_homo_sapiens)\n",
    "df_body_part_homo_sapiens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we divide the dataframe into non up to pentahydroxylated bile acids\n",
    "\n",
    "merged_tables_non_homo_sapiens = merged_tables_all_homo_sapiens.loc[merged_tables_all_homo_sapiens['Bile acid'] == 'Nonhydroxy']\n",
    "merged_tables_mono_homo_sapiens = merged_tables_all_homo_sapiens.loc[merged_tables_all_homo_sapiens['Bile acid'] == 'Monohydroxy']\n",
    "merged_tables_di_homo_sapiens = merged_tables_all_homo_sapiens.loc[merged_tables_all_homo_sapiens['Bile acid'] == 'Dihydroxy']\n",
    "merged_tables_tri_homo_sapiens = merged_tables_all_homo_sapiens.loc[merged_tables_all_homo_sapiens['Bile acid'] == 'Trihydroxy']\n",
    "merged_tables_tetra_homo_sapiens = merged_tables_all_homo_sapiens.loc[merged_tables_all_homo_sapiens['Bile acid'] == 'Tetrahydroxy']\n",
    "merged_tables_penta_homo_sapiens = merged_tables_all_homo_sapiens.loc[merged_tables_all_homo_sapiens['Bile acid'] == 'Pentahydroxy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we create new dataframes with the counts of each time a body part occur. This is the total_deltas column.\n",
    "#With this, we have the information of how common these deltas are (not necessarily the diversity)\n",
    "merged_tables_non_homo_sapiens_BodyPartName = merged_tables_non_homo_sapiens['UBERONBodyPartName'].value_counts().rename_axis('UBERONBodyPartName').reset_index(name='Non_total_deltas')\n",
    "merged_tables_mono_homo_sapiens_BodyPartName = merged_tables_mono_homo_sapiens['UBERONBodyPartName'].value_counts().rename_axis('UBERONBodyPartName').reset_index(name='Mono_total_deltas')\n",
    "merged_tables_di_homo_sapiens_BodyPartName = merged_tables_di_homo_sapiens['UBERONBodyPartName'].value_counts().rename_axis('UBERONBodyPartName').reset_index(name='Di_total_deltas')\n",
    "merged_tables_tri_homo_sapiens_BodyPartName = merged_tables_tri_homo_sapiens['UBERONBodyPartName'].value_counts().rename_axis('UBERONBodyPartName').reset_index(name='Tri_total_deltas')\n",
    "merged_tables_tetra_homo_sapiens_BodyPartName = merged_tables_tetra_homo_sapiens['UBERONBodyPartName'].value_counts().rename_axis('UBERONBodyPartName').reset_index(name='Tetra_total_deltas')\n",
    "merged_tables_penta_homo_sapiens_BodyPartName = merged_tables_penta_homo_sapiens['UBERONBodyPartName'].value_counts().rename_axis('UBERONBodyPartName').reset_index(name='Penta_total_deltas')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#then, we merge all these tables (has to be one by one)\n",
    "\n",
    "df_body_part_homo_sapiens = pd.merge(merged_tables_non_homo_sapiens_BodyPartName,\n",
    "                              merged_tables_mono_homo_sapiens_BodyPartName, on = \"UBERONBodyPartName\", how = \"outer\")\n",
    "df_body_part_homo_sapiens.fillna(\"N/A\", inplace = True)\n",
    "\n",
    "df_body_part_homo_sapiens = pd.merge(df_body_part_homo_sapiens,\n",
    "                              merged_tables_di_homo_sapiens_BodyPartName, on = \"UBERONBodyPartName\", how = \"outer\")\n",
    "df_body_part_homo_sapiens.fillna(\"N/A\", inplace = True)\n",
    "\n",
    "df_body_part_homo_sapiens = pd.merge(df_body_part_homo_sapiens,\n",
    "                              merged_tables_tri_homo_sapiens_BodyPartName, on = \"UBERONBodyPartName\", how = \"outer\")\n",
    "df_body_part_homo_sapiens.fillna(\"N/A\", inplace = True)\n",
    "\n",
    "df_body_part_homo_sapiens = pd.merge(df_body_part_homo_sapiens,\n",
    "                              merged_tables_tetra_homo_sapiens_BodyPartName, on = \"UBERONBodyPartName\", how = \"outer\")\n",
    "df_body_part_homo_sapiens.fillna(\"N/A\", inplace = True)\n",
    "\n",
    "df_body_part_homo_sapiens = pd.merge(df_body_part_homo_sapiens,\n",
    "                              merged_tables_penta_homo_sapiens_BodyPartName, on = \"UBERONBodyPartName\", how = \"outer\")\n",
    "df_body_part_homo_sapiens.fillna(\"N/A\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_body_part_homo_sapiens.to_csv('/Users/helenarusso/Downloads/body_parts_homo_sapiens_total_2decimals_fastmasst_non_refined.tsv', sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
